import os
from typing import Any, TypedDict

from langchain_core.messages import (
    AIMessage,
    ToolMessage,
    convert_to_openai_messages,
)
from langchain_core.tools import Tool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.runtime import Runtime

from utils.listener import MyCustomHandler
from utils.logger import setup_logger

# Set up logger for this module
logger = setup_logger(__name__)

LLM_URL = os.environ.get("LLM_URL", "http://agentgateway:3000/bedrock")
MCP_URL = os.environ.get("MCP_URL", "http://agentgateway:3000/graph")


# State class with proper typing
class AgentState(TypedDict):
    messages: list
    initial_prompt: list


class Context(TypedDict):
    llm: Any  # Using Any for flexibility
    toolNode: ToolNode
    tools: list[Tool]


# Setup function that can be awaited as the MCP Client is async
async def setup_tools_async():
    from langchain_mcp_adapters.client import MultiServerMCPClient

    client = MultiServerMCPClient(
        {
            "math": {
                "url": MCP_URL,
                "transport": "streamable_http",
            }
        }
    )
    # Initialize the MCP tool
    tools = await client.get_tools()
    # add another Standalone Tool next to remote MCP Server
    from tools.standalone import prefix

    tools.append(prefix)
    return tools


# Async node function that uses the pre-initialized components
async def agent_node(state: AgentState, runtime: Runtime[Context]) -> AgentState:
    llm = runtime.context.get("llm", None)
    # The convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.
    oai_messages = {"messages": convert_to_openai_messages(state["initial_prompt"])}
    # Use the pre-initialized agent
    logger.info("Agent input: %s", oai_messages)  # Changed from print
    response = await llm.ainvoke(state["initial_prompt"])
    logger.debug(
        "Response type: %s", response.type if hasattr(response, "type") else "unknown"
    )  # Changed from print
    logger.debug(
        "Tool calls: %s",
        response.tool_calls if hasattr(response, "tool_calls") else "none",
    )  # Changed from print
    logger.info(
        "Response content: %s",
        response.content if hasattr(response, "content") else str(response),
    )  # Changed from print

    # Check if the response has tool calls
    if not hasattr(response, "tool_calls") or not response.tool_calls:
        logger.warning("No tool calls were generated by the LLM")  # Changed from print
    state["messages"].append(response)  # Keep history
    return {"messages": state["messages"]}  # Return updated history


# Tool node function
async def tool_node(state: AgentState, runtime: Runtime[Context]) -> AgentState:
    # Get the last message (which should have tool calls)
    last_message = state["messages"][-1]
    tool_node = runtime.context.get("toolNode")

    # Create a proper input for the tool node
    if not hasattr(last_message, "tool_calls") and isinstance(last_message, dict):
        # If it's a dict with tool_calls field
        if "tool_calls" in last_message:
            # Convert the dict to a proper AIMessage
            tool_message = AIMessage(content="", tool_calls=last_message["tool_calls"])
            tool_input = {"messages": [tool_message]}
        else:
            raise ValueError(f"Last message doesn't have tool_calls: {last_message}")
    else:
        # If it's already a message object with tool_calls
        tool_input = {"messages": [last_message]}

    # Debug output to see what we're sending
    logger.debug("Tool input: %s", tool_input)  # Changed from print

    # Execute the tool
    tool_result = await tool_node.ainvoke(tool_input)

    # Add the tool result to messages
    if "messages" in tool_result and tool_result["messages"]:
        state["messages"].append(tool_result["messages"][-1])
    else:
        logger.warning(
            "Unexpected tool result format: %s", tool_result
        )  # Changed from print
        # Create a default tool message if needed
        state["messages"].append(
            ToolMessage(
                content="Tool execution completed but returned unexpected format"
            )
        )

    return {"messages": state["messages"]}


# Route function to decide what to do next
def should_call_tool(state: AgentState) -> str:
    """
    Determines if we should continue with the agent or use tools
    """
    logger.debug("should_continue called with state: %s", state)  # Changed from print

    messages = state.get("messages")
    if not messages:
        logger.debug("No messages, returning 'agent'")  # Changed from print
        return "agent"

    last_message = messages[-1]
    logger.debug("Last message: %s", last_message)  # Changed from print

    # Check if the agent wants to use tools
    has_tool_calls = hasattr(last_message, "tool_calls") and last_message.tool_calls
    logger.debug("Has tool calls: %s", has_tool_calls)  # Changed from print

    if has_tool_calls:
        logger.debug("Routing to 'tools'")  # Changed from print
        return "tools"
    else:
        logger.debug("Routing to 'end'")  # Changed from print
        return "end"


async def get_components():
    # Initialize ChatOpenAI to use AgentGateway
    llm = ChatOpenAI(
        model="gpt-4o",  # Use a dummy model name; AgentGateway handles routing
        base_url=LLM_URL,  # AgentGateway endpoint
        api_key="sk-dummy",  # Dummy key (if required)
        temperature=0,
        callbacks=[MyCustomHandler()],  # add a Listener to the LLM Agent
    )
    tools = await setup_tools_async()
    return llm, tools


# Build LangGraph workflow
builder = StateGraph(AgentState, context_schema=Context)
builder.add_node("agent", agent_node)
builder.add_node("tools", tool_node)
builder.add_edge(START, "agent")
builder.add_conditional_edges(
    "agent",
    should_call_tool,
    {
        "tools": "tools",
        "end": END,
        "agent": "agent",  # Allow looping back to agent if needed
    },
)
builder.add_conditional_edges(
    "tools",
    should_call_tool,
    {"end": END, "agent": "agent"},  # Allow looping back to agent if needed
)
graph = builder.compile()
graph.get_graph().draw_mermaid_png(output_file_path="./data/graphs/graph.png")
